{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports and setup\nimport os, json, re, random\nfrom collections import Counter, defaultdict\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import transforms, models\nfrom tqdm.auto import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nseed = 42\nrandom.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\nprint(f\"Device: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:12:36.186773Z","iopub.execute_input":"2026-01-04T12:12:36.187372Z","iopub.status.idle":"2026-01-04T12:12:36.194837Z","shell.execute_reply.started":"2026-01-04T12:12:36.187341Z","shell.execute_reply":"2026-01-04T12:12:36.194135Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Config: Kaggle paths and hyperparameters\n# Updated to match dataset structure: /kaggle/input/coco-2017-dataset/coco2017\nDATA_ROOT = \"/kaggle/input/coco-2017-dataset/coco2017\"\nTRAIN_IMAGES_DIR = os.path.join(DATA_ROOT, \"train2017\")\nCAPTIONS_PATH = os.path.join(DATA_ROOT, \"annotations\", \"captions_train2017.json\")\n\n# Training hyperparameters\nNUM_EPOCHS = 5\nBATCH_SIZE = 32\nLEARNING_RATE = 1e-4\n\n# Captioning hyperparameters\nEMBED_DIM = 256\nHIDDEN_DIM = 512\nNUM_LSTM_LAYERS = 1\nMAX_SEQ_LEN = 20\nMIN_FREQ = 5\n\n# Optional: limit number of samples for quick runs (set to None for all)\nMAX_SAMPLES = None\n\n# Defined action classes\nACTION_CLASSES = [\"walking\", \"running\", \"sitting\", \"standing\", \"riding\", \"playing\", \"eating\"]\n\n# Verify expected paths exist\nprint(\"Images dir exists:\", os.path.isdir(TRAIN_IMAGES_DIR))\nprint(\"Captions file exists:\", os.path.isfile(CAPTIONS_PATH))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:12:36.195983Z","iopub.execute_input":"2026-01-04T12:12:36.196235Z","iopub.status.idle":"2026-01-04T12:12:36.210939Z","shell.execute_reply.started":"2026-01-04T12:12:36.196213Z","shell.execute_reply":"2026-01-04T12:12:36.210283Z"}},"outputs":[{"name":"stdout","text":"Images dir exists: True\nCaptions file exists: True\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Action verb mapping and tokenizer\n# Simple rule-based verb mapping without external NLP libraries\nVERB_TO_ACTION = {\n    \"walk\": \"walking\", \"walking\": \"walking\", \"walks\": \"walking\", \"walked\": \"walking\",\n    \"run\": \"running\", \"running\": \"running\", \"runs\": \"running\", \"ran\": \"running\",\n    \"sit\": \"sitting\", \"sitting\": \"sitting\", \"sits\": \"sitting\", \"sat\": \"sitting\",\n    \"stand\": \"standing\", \"standing\": \"standing\", \"stands\": \"standing\", \"stood\": \"standing\",\n    \"ride\": \"riding\", \"riding\": \"riding\", \"rides\": \"riding\", \"rode\": \"riding\",\n    \"play\": \"playing\", \"playing\": \"playing\", \"plays\": \"playing\", \"played\": \"playing\",\n    \"eat\": \"eating\", \"eating\": \"eating\", \"eats\": \"eating\", \"ate\": \"eating\"\n}\n\ndef tokenize(text):\n    text = text.lower()\n    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n    tokens = [t for t in text.split() if t]\n    return tokens\n\ndef extract_action(tokens):\n    for t in tokens:\n        if t in VERB_TO_ACTION:\n            return VERB_TO_ACTION[t]\n    return None\n\naction_to_idx = {a: i for i, a in enumerate(ACTION_CLASSES)}\nidx_to_action = {i: a for a, i in action_to_idx.items()}\nprint(\"Action to index:\", action_to_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:12:36.211720Z","iopub.execute_input":"2026-01-04T12:12:36.211969Z","iopub.status.idle":"2026-01-04T12:12:36.229530Z","shell.execute_reply.started":"2026-01-04T12:12:36.211938Z","shell.execute_reply":"2026-01-04T12:12:36.228858Z"}},"outputs":[{"name":"stdout","text":"Action to index: {'walking': 0, 'running': 1, 'sitting': 2, 'standing': 3, 'riding': 4, 'playing': 5, 'eating': 6}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Parse COCO captions and filter images by defined actions\nassert os.path.isfile(CAPTIONS_PATH), \"captions_train2017.json not found. Check DATA_ROOT.\"\nprint(\"Loading COCO captions...\")\nwith open(CAPTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n    caps_data = json.load(f)\n\nid_to_filename = {img[\"id\"]: img[\"file_name\"] for img in caps_data[\"images\"]}\nid_to_caps = defaultdict(list)\nfor ann in caps_data[\"annotations\"]:\n    id_to_caps[ann[\"image_id\"]].append(ann[\"caption\"])\n\nitems = []\nclass_counts = Counter()\nfor image_id, caps in tqdm(id_to_caps.items(), total=len(id_to_caps), desc=\"Filtering images\"):\n    fn = id_to_filename.get(image_id)\n    if not fn:\n        continue\n    img_path = os.path.join(TRAIN_IMAGES_DIR, fn)\n    if not os.path.isfile(img_path):\n        continue\n    chosen = None\n    chosen_action = None\n    for c in caps:\n        toks = tokenize(c)\n        act = extract_action(toks)\n        if act is not None:\n            chosen = toks\n            chosen_action = act\n            break\n    if chosen is None:\n        continue\n    items.append({\"image_path\": img_path, \"caption_tokens\": chosen, \"action\": chosen_action})\n    class_counts[chosen_action] += 1\n\nif MAX_SAMPLES is not None:\n    items = items[:MAX_SAMPLES]\n\nprint(f\"Filtered samples: {len(items)}\")\nprint(\"Class distribution:\", dict(class_counts))\nassert len(items) > 0, \"No samples found with the defined actions.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:12:36.230648Z","iopub.execute_input":"2026-01-04T12:12:36.230888Z","iopub.status.idle":"2026-01-04T12:17:03.079659Z","shell.execute_reply.started":"2026-01-04T12:12:36.230870Z","shell.execute_reply":"2026-01-04T12:17:03.078918Z"}},"outputs":[{"name":"stdout","text":"Loading COCO captions...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filtering images:   0%|          | 0/118287 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ca2e15aa5704c2db2acd93731f85e55"}},"metadata":{}},{"name":"stdout","text":"Filtered samples: 80449\nClass distribution: {'sitting': 31978, 'riding': 9308, 'eating': 2872, 'walking': 6358, 'standing': 22414, 'playing': 6226, 'running': 1293}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Vocabulary building\nclass Vocabulary:\n    def __init__(self, min_freq=1):\n        self.min_freq = min_freq\n        self.specials = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"]\n        self.stoi = {}\n        self.itos = []\n        self.pad_token = \"<PAD>\"\n        self.sos_token = \"<SOS>\"\n        self.eos_token = \"<EOS>\"\n        self.unk_token = \"<UNK>\"\n\n    def build(self, token_lists):\n        freq = Counter()\n        for toks in token_lists:\n            freq.update(toks)\n        self.itos = list(self.specials)\n        for tok, c in freq.items():\n            if c >= self.min_freq:\n                self.itos.append(tok)\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n        self.pad_id = self.stoi[self.pad_token]\n        self.sos_id = self.stoi[self.sos_token]\n        self.eos_id = self.stoi[self.eos_token]\n        self.unk_id = self.stoi[self.unk_token]\n\n    def numericalize(self, tokens):\n        return [self.stoi.get(t, self.unk_id) for t in tokens]\n\n    def encode(self, tokens, max_len):\n        inp = [self.sos_token] + tokens\n        tgt = tokens + [self.eos_token]\n        in_ids = self.numericalize(inp)\n        tg_ids = self.numericalize(tgt)\n        # pad/truncate\n        in_ids = (in_ids + [self.pad_id] * max_len)[:max_len]\n        tg_ids = (tg_ids + [self.pad_id] * max_len)[:max_len]\n        return in_ids, tg_ids\n\ntoken_lists = [it[\"caption_tokens\"] for it in items]\nvocab = Vocabulary(min_freq=MIN_FREQ)\nvocab.build(token_lists)\nVOCAB_SIZE = len(vocab.itos)\nprint(\"Vocab size:\", VOCAB_SIZE)\nprint(\"PAD id:\", vocab.pad_id, \"SOS id:\", vocab.sos_id, \"EOS id:\", vocab.eos_id, \"UNK id:\", vocab.unk_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:03.080533Z","iopub.execute_input":"2026-01-04T12:17:03.080756Z","iopub.status.idle":"2026-01-04T12:17:03.215484Z","shell.execute_reply.started":"2026-01-04T12:17:03.080735Z","shell.execute_reply":"2026-01-04T12:17:03.214749Z"}},"outputs":[{"name":"stdout","text":"Vocab size: 3257\nPAD id: 0 SOS id: 1 EOS id: 2 UNK id: 3\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# PyTorch Dataset\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n])\n\nclass COCOActionCaptionDataset(Dataset):\n    def __init__(self, items, vocab, transform, action_to_idx, max_len):\n        self.items = items\n        self.vocab = vocab\n        self.transform = transform\n        self.action_to_idx = action_to_idx\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.items)\n\n    def __getitem__(self, idx):\n        it = self.items[idx]\n        img = Image.open(it[\"image_path\"]).convert(\"RGB\")\n        img_t = self.transform(img)\n        action_idx = self.action_to_idx[it[\"action\"]]\n        in_ids, tg_ids = self.vocab.encode(it[\"caption_tokens\"], self.max_len)\n        return (\n            img_t,\n            torch.tensor(action_idx, dtype=torch.long),\n            torch.tensor(in_ids, dtype=torch.long),\n            torch.tensor(tg_ids, dtype=torch.long),\n        )\n\ndataset = COCOActionCaptionDataset(items, vocab, transform, action_to_idx, MAX_SEQ_LEN)\nprint(\"Dataset size:\", len(dataset))\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:03.217021Z","iopub.execute_input":"2026-01-04T12:17:03.217340Z","iopub.status.idle":"2026-01-04T12:17:03.225233Z","shell.execute_reply.started":"2026-01-04T12:17:03.217316Z","shell.execute_reply":"2026-01-04T12:17:03.224502Z"}},"outputs":[{"name":"stdout","text":"Dataset size: 80449\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Model: Shared ResNet-50 backbone + Action head + Captioning LSTM\nclass SharedResNet50(nn.Module):\n    def __init__(self):\n        super().__init__()\n        try:\n            resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n        except Exception:\n            resnet = models.resnet50(pretrained=True)\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # up to avgpool\n        self.out_dim = 2048\n    def forward(self, x):\n        f = self.backbone(x)  # (N, 2048, 1, 1)\n        return f.view(f.size(0), -1)\n\nclass ActionHead(nn.Module):\n    def __init__(self, in_dim, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass CaptionDecoder(nn.Module):\n    def __init__(self, in_dim, vocab_size, embed_dim, hidden_dim, num_layers, pad_id):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n        self.to_h0 = nn.Linear(in_dim, hidden_dim)\n        self.to_c0 = nn.Linear(in_dim, hidden_dim)\n        self.out = nn.Linear(hidden_dim, vocab_size)\n        self.num_layers = num_layers\n    def forward(self, img_feat, captions_in):\n        emb = self.embed(captions_in)\n        h0 = torch.tanh(self.to_h0(img_feat))\n        c0 = torch.tanh(self.to_c0(img_feat))\n        h0 = h0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n        c0 = c0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n        out_seq, _ = self.lstm(emb, (h0, c0))\n        logits = self.out(out_seq)  # (N, T, V)\n        return logits\n\nclass MultiTaskModel(nn.Module):\n    def __init__(self, num_actions, vocab_size, embed_dim, hidden_dim, num_layers, pad_id):\n        super().__init__()\n        self.cnn = SharedResNet50()\n        self.action_head = ActionHead(self.cnn.out_dim, num_actions)\n        self.caption_head = CaptionDecoder(self.cnn.out_dim, vocab_size, embed_dim, hidden_dim, num_layers, pad_id)\n    def forward(self, images, captions_in):\n        img_feat = self.cnn(images)\n        action_logits = self.action_head(img_feat)\n        caption_logits = self.caption_head(img_feat, captions_in)\n        return action_logits, caption_logits\n\nmodel = MultiTaskModel(num_actions=len(ACTION_CLASSES), vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM, num_layers=NUM_LSTM_LAYERS, pad_id=vocab.pad_id)\nmodel = model.to(device)\nprint(model.__class__.__name__, \"initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:03.226004Z","iopub.execute_input":"2026-01-04T12:17:03.226238Z","iopub.status.idle":"2026-01-04T12:17:04.736758Z","shell.execute_reply.started":"2026-01-04T12:17:03.226217Z","shell.execute_reply":"2026-01-04T12:17:04.736074Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 97.8M/97.8M [00:00<00:00, 203MB/s]\n","output_type":"stream"},{"name":"stdout","text":"MultiTaskModel initialized\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Training loop with combined loss\naction_criterion = nn.CrossEntropyLoss()\ncaption_criterion = nn.CrossEntropyLoss(ignore_index=vocab.pad_id)\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\ndef train_one_epoch(epoch):\n    model.train()\n    total_loss = 0.0\n    total_action = 0.0\n    total_caption = 0.0\n    n_batches = 0\n    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n    for imgs, act_targets, cap_in, cap_tg in pbar:\n        imgs = imgs.to(device, non_blocking=True)\n        act_targets = act_targets.to(device, non_blocking=True)\n        cap_in = cap_in.to(device, non_blocking=True)\n        cap_tg = cap_tg.to(device, non_blocking=True)\n        optimizer.zero_grad()\n        act_logits, cap_logits = model(imgs, cap_in)\n        act_loss = action_criterion(act_logits, act_targets)\n        V = cap_logits.size(-1)\n        cap_loss = caption_criterion(cap_logits.view(-1, V), cap_tg.view(-1))\n        loss = 0.4 * act_loss + 0.6 * cap_loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        total_action += act_loss.item()\n        total_caption += cap_loss.item()\n        n_batches += 1\n        pbar.set_postfix({\"total\": f\"{loss.item():.4f}\", \"action\": f\"{act_loss.item():.4f}\", \"caption\": f\"{cap_loss.item():.4f}\"})\n    avg_loss = total_loss / max(1, n_batches)\n    avg_action = total_action / max(1, n_batches)\n    avg_caption = total_caption / max(1, n_batches)\n    print(f\"Epoch {epoch}: Total {avg_loss:.4f} | Action {avg_action:.4f} | Caption {avg_caption:.4f}\")\n\nfor ep in range(1, NUM_EPOCHS + 1):\n    train_one_epoch(ep)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T12:17:04.737803Z","iopub.execute_input":"2026-01-04T12:17:04.738388Z","iopub.status.idle":"2026-01-04T13:26:56.785041Z","shell.execute_reply.started":"2026-01-04T12:17:04.738360Z","shell.execute_reply":"2026-01-04T13:26:56.784230Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/2515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1: Total 2.3894 | Action 0.8889 | Caption 3.3897\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/2515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2: Total 1.7731 | Action 0.6710 | Caption 2.5079\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/2515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3: Total 1.5476 | Action 0.4741 | Caption 2.2633\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4:   0%|          | 0/2515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4: Total 1.3958 | Action 0.3042 | Caption 2.1235\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5:   0%|          | 0/2515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5: Total 1.2918 | Action 0.1966 | Caption 2.0219\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Save model weights, vocabulary, and action labels\nos.makedirs(\"model_artifacts\", exist_ok=True)\nsave_path = os.path.join(\"model_artifacts\", \"multitask_cnn_lstm.pth\")\ntorch.save({\n    \"model_state_dict\": model.state_dict(),\n    \"config\": {\n        \"embed_dim\": EMBED_DIM,\n        \"hidden_dim\": HIDDEN_DIM,\n        \"num_lstm_layers\": NUM_LSTM_LAYERS,\n        \"max_seq_len\": MAX_SEQ_LEN,\n        \"num_actions\": len(ACTION_CLASSES),\n        \"vocab_size\": VOCAB_SIZE,\n        \"pad_id\": vocab.pad_id,\n        \"sos_id\": vocab.sos_id,\n        \"eos_id\": vocab.eos_id,\n        \"unk_id\": vocab.unk_id\n    }\n}, save_path)\n\nwith open(os.path.join(\"model_artifacts\", \"vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n    json.dump({\n        \"itos\": vocab.itos,\n        \"stoi\": vocab.stoi\n    }, f)\n\nwith open(os.path.join(\"model_artifacts\", \"action_labels.json\"), \"w\", encoding=\"utf-8\") as f:\n    json.dump(ACTION_CLASSES, f)\n\nprint(\"Saved:\", save_path)\nprint(\"Artifacts in ./model_artifacts\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T13:26:56.786789Z","iopub.execute_input":"2026-01-04T13:26:56.787048Z","iopub.status.idle":"2026-01-04T13:26:57.008527Z","shell.execute_reply.started":"2026-01-04T13:26:56.787016Z","shell.execute_reply":"2026-01-04T13:26:57.007729Z"}},"outputs":[{"name":"stdout","text":"Saved: model_artifacts/multitask_cnn_lstm.pth\nArtifacts in ./model_artifacts\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Inference: load artifacts and test on a sample image\n\nARTIFACT_DIR = \"model_artifacts\"  # On Kaggle, this is under /kaggle/working/model_artifacts\nCKPT_PATH = os.path.join(ARTIFACT_DIR, \"multitask_cnn_lstm.pth\")\nVOCAB_PATH = os.path.join(ARTIFACT_DIR, \"vocab.json\")\nACTIONS_PATH = os.path.join(ARTIFACT_DIR, \"action_labels.json\")\n\nassert os.path.isfile(CKPT_PATH), f\"Checkpoint not found at {CKPT_PATH}\"\nassert os.path.isfile(VOCAB_PATH), f\"Vocab not found at {VOCAB_PATH}\"\nassert os.path.isfile(ACTIONS_PATH), f\"Action labels not found at {ACTIONS_PATH}\"\n\nwith open(VOCAB_PATH, \"r\", encoding=\"utf-8\") as f:\n    vdata = json.load(f)\nwith open(ACTIONS_PATH, \"r\", encoding=\"utf-8\") as f:\n    action_classes = json.load(f)\n\n# Rebuild vocab object\nvocab = Vocabulary(min_freq=1)\nvocab.itos = vdata[\"itos\"]\nvocab.stoi = {k: int(v) if isinstance(v, str) and v.isdigit() else v for k, v in vdata[\"stoi\"].items()}\nvocab.pad_id = vocab.stoi[vocab.pad_token]\nvocab.sos_id = vocab.stoi[vocab.sos_token]\nvocab.eos_id = vocab.stoi[vocab.eos_token]\nvocab.unk_id = vocab.stoi[vocab.unk_token]\n\n# Load checkpoint and rebuild model\nckpt = torch.load(CKPT_PATH, map_location=device)\nconf = ckpt[\"config\"]\nmodel = MultiTaskModel(\n    num_actions=len(action_classes),\n    vocab_size=conf[\"vocab_size\"],\n    embed_dim=conf[\"embed_dim\"],\n    hidden_dim=conf[\"hidden_dim\"],\n    num_layers=conf[\"num_lstm_layers\"],\n    pad_id=conf[\"pad_id\"],\n).to(device)\nmodel.load_state_dict(ckpt[\"model_state_dict\"])  # type: ignore\nmodel.eval()\n\n# Pick a sample image from val/train/test folders\nsample_dirs = [\n    os.path.join(DATA_ROOT, \"val2017\"),\n    os.path.join(DATA_ROOT, \"train2017\"),\n    os.path.join(DATA_ROOT, \"test2017\"),\n]\nexisting_dirs = [d for d in sample_dirs if os.path.isdir(d)]\nassert len(existing_dirs) > 0, \"No image folders found for testing.\"\nimg_dir = existing_dirs[0]\nimgs = [f for f in os.listdir(img_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\nassert len(imgs) > 0, f\"No images found in {img_dir}\"\n# sample_img_path = os.path.join(img_dir, random.choice(imgs))\nsample_img_path = \"/kaggle/input/coco-2017-dataset/coco2017/test2017/000000000016.jpg\"\nprint(\"Testing image:\", sample_img_path)\n\n# Preprocess and run model\nimg = Image.open(sample_img_path).convert(\"RGB\")\nimg_t = transform(img).unsqueeze(0).to(device)\nwith torch.no_grad():\n    action_logits, _ = model(img_t, torch.tensor([[vocab.sos_id]], device=device))\n    action_probs = F.softmax(action_logits, dim=-1).squeeze(0).cpu()\n\n# Top-5 actions\ntopk = torch.topk(action_probs, k=min(5, len(action_classes)))\nprint(\"Top action predictions:\")\nfor p, i in zip(topk.values.tolist(), topk.indices.tolist()):\n    print(f\"  {action_classes[i]}: {p:.3f}\")\n\n# Greedy caption generation\ndef generate_caption(model, img_t, vocab, max_len):\n    with torch.no_grad():\n        img_feat = model.cnn(img_t)\n        h0 = torch.tanh(model.caption_head.to_h0(img_feat))\n        c0 = torch.tanh(model.caption_head.to_c0(img_feat))\n        h = h0.unsqueeze(0).repeat(model.caption_head.num_layers, 1, 1)\n        c = c0.unsqueeze(0).repeat(model.caption_head.num_layers, 1, 1)\n        cur = torch.tensor([[vocab.sos_id]], device=img_t.device)\n        out_tokens = []\n        for _ in range(max_len):\n            emb = model.caption_head.embed(cur)\n            y, (h, c) = model.caption_head.lstm(emb, (h, c))\n            logits = model.caption_head.out(y[:, -1, :])\n            nxt = torch.argmax(logits, dim=-1).item()\n            if nxt == vocab.eos_id:\n                break\n            out_tokens.append(nxt)\n            cur = torch.tensor([[nxt]], device=img_t.device)\n        words = [vocab.itos[i] if i < len(vocab.itos) else \"<UNK>\" for i in out_tokens]\n        return \" \".join(words)\n\ncaption = generate_caption(model, img_t, vocab, max_len=conf.get(\"max_seq_len\", 20))\nprint(\"Generated caption:\", caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T14:03:41.413723Z","iopub.execute_input":"2026-01-04T14:03:41.414544Z","iopub.status.idle":"2026-01-04T14:03:42.135352Z","shell.execute_reply.started":"2026-01-04T14:03:41.414496Z","shell.execute_reply":"2026-01-04T14:03:42.134598Z"}},"outputs":[{"name":"stdout","text":"Testing image: /kaggle/input/coco-2017-dataset/coco2017/test2017/000000000016.jpg\nTop action predictions:\n  playing: 0.588\n  standing: 0.392\n  running: 0.020\n  walking: 0.001\n  sitting: 0.000\nGenerated caption: a man holding a bat while standing on a field\n","output_type":"stream"}],"execution_count":15}]}